demo()
demo(nlm)
q()
# Exercício 1 - Crie um vetor com 12 números inteiros
vetor = c(1,2,3,4,5,6,7,8,9,10,11,12)
# Exercício 2 - Crie uma matriz com 4 linhas e 4 colunas preenchida com números inteiros
m = matrix(data = c(1,2,3,4,5,6,7,8,9,10,11,12,
13,14,15,16,33,44,22,33,11,1,0,2,
2,44,55,35,45,7,3,6,1,2,3,4,
7,2,6,2,9,3,36,57,3,8,58,23), ncol = 4)
m
# Exercício 3 - Crie uma lista unindo o vetor e matriz criados anteriormente
new_matrix = cbind(vetor, m)
new_matrix
# Exercício 4 - Usando a função read.table() leia o arquivo do link abaixo para uma dataframe
the_table = read.table("https://grodri.github.io/datasets/effort.dat")
the_table
# Exercício 5 - Transforme o dataframe anterior, em um dataframe nomeado com os seguintes
#labels:
new_labels <-  c("config", "esfc", "chang")
colnames(the_table) <- new_labels
the_table
# Exercício 6 - Imprima na tela o dataframe iris, verifique quantas dimensões existem no
the_table = read.table("https://grodri.github.io/datasets/iris.dat")
# Exercício 6 - Imprima na tela o dataframe iris, verifique quantas dimensões existem no
iris_dataset = data(iris)
iris_dataset
head(iris_dataset)
data(iris)
head(data(iris))
iris
class(iris)
dim(iris)
summary(iris)
str(iris)
# Exercicio 7 - Crie um plot simples usando as duas primeiras colunas do dataframe iris
plot(iris$Sepal.Length, iris$Sepal.Width)
# Exercicio 8 - Usando s função subset, crie um novo dataframe com o conjunto de dados do dataframe iris em que Sepal.Length > 7
?subset
iris1 <- subset(iris, Sepal.Length > 7)
iris1
# Exercicio 9 - Crie um dataframe que seja cópia do dataframe iris e usando a função slice(), divida o dataframe em um subset de 15 linhas
# Dica 1: voce vai ter que instalar e carregar o pacote dplyr
# Dica 2: Consulte o help para aprender como usar a funcao slice()
novo_iris <- iris
novo_iris
install.packages("dplyr")
library(dplyr)
?slice
slice(novo_iris, 1:15)
# Exercicio 10 - Use a funcao filter no seu novo dataframe criado no item anterior e retorne apenas valores em que Sepal.Length > 6
# Dica: Use o RSiteSearch para aprender como usar a funcao filter
RSiteSearch('filter')
filter(novo_iris, Sepal.Length > 7)
filter(novo_iris, Sepal.Length > 7)
getwd()
load("~/Documents/data_science/big_data_analitycs_with_r_and_azure/r_part1/rfundamentos.R")
setwd("~/Documents/data_science/big_data_analitycs_with_r_and_azure")
load("~/Documents/data_science/big_data_analitycs_with_r_and_azure/r_part2/part2.R")
search()
install.packages(c("ggvis", "tm", "dplyr"))
# remove o import
detach(pachage:dplyr)
# listar o pacote
ls(pos="package:tm")
# datasets predefinidos
data()
plot(iris)
hist(iris)
hist(lynx)
sum(iris$Sepal.Length)
sum(Sepal.length)
# o dataset pode ser conectado a area de trabalho:
attach(iris)
sum(Sepal.length)
# o dataset pode ser conectado a area de trabalho:
attach(iris)
sum(Sepal.length)
str <- c("expressoes", "regulares", "em R", "permitem busca por padroes")
grep("ex", str, value = F)
grep("ex", str, value = T)
install.packagers(c("tm", "SnowballC", "worldcloud", "RColorBrewer"))
install.packages(c("tm", "SnowballC", "worldcloud", "RColorBrewer"))
install.packages(c("slam", "xml2","tm", "SnowballC", "worldcloud", "RColorBrewer"))
install.packages(c("tm", "SnowballC", "worldcloud", "RColorBrewer"))
install.packages(c("tm", "SnowballC", "worldcloud", "RColorBrewer"))
df <- read.csv2("questoes.csv", stringsAsFactors = FALSE)
setwd("~/Documents/data_science/big_data_analitycs_with_r_and_azure/r_part_3")
df <- read.csv2("questoes.csv", stringsAsFactors = FALSE)
head(df)
#Criando um corpus
dfCorpus <- Corpus(VectorSource(df$Question))
library(tm)
library(SnowballC)
library(wordcloud)
install.packages(c("tm", "SnowballC", "wordcloud", "RColorBrewer"))
install.packages(c("tm", "SnowballC", "wordcloud", "RColorBrewer"))
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
#Criando um corpus
dfCorpus <- Corpus(VectorSource(df$Question))
class(dfCorpus)
# Convertendo corpus em um documento de texto plano
dfCorpus <- tm_map(dfCorpus, PlainTextDocument)
# Remove Pontuação
dfCorpus <- tm_map(dfCorpus, removePunctuation)
# Remover palavras especificas em ingles
dfCorpus <- tm_map(dfCorpus, removeWords, stopwords("english"))
head(dfCorpus)
# converter palavras similares em unicas
dfCorpus <- tm_map(dfCorpus, stemDocument)
#wordcount
wordcloud(dfCorpus, max.words = 100, random.order = FALSE)
#removendo palavras especificas
dfCorpus <- tm_map(dfCorpus, removeWords, c("the", "this", stopwords("english")))
#wordcount
wordcloud(dfCorpus, max.words = 100, random.order = FALSE)
df <- read.csv2("questoes.csv", stringsAsFactors = FALSE)
#Criando um corpus
dfCorpus <- Corpus(VectorSource(df$Question))
class(dfCorpus)
# Convertendo corpus em um documento de texto plano
dfCorpus <- tm_map(dfCorpus, PlainTextDocument)
# Convertendo corpus em um documento de texto plano
dfCorpus <- tm_map(dfCorpus, PlainTextDocument)
# Remove Pontuação
dfCorpus <- tm_map(dfCorpus, removePunctuation)
# Remover palavras especificas em ingles
dfCorpus <- tm_map(dfCorpus, removeWords, stopwords("english"))
# converter palavras similares em unicas
dfCorpus <- tm_map(dfCorpus, stemDocument)
#removendo palavras especificas
dfCorpus <- tm_map(dfCorpus, removeWords, c("the", "this", stopwords("english")))
#wordcount
wordcloud(dfCorpus, max.words = 100, random.order = FALSE)
df <- read.csv2("questoes.csv", stringsAsFactors = FALSE)
head(df)
#Criando um corpus
dfCorpus <- Corpus(VectorSource(df$Question))
class(dfCorpus)
# Convertendo corpus em um documento de texto plano
dfCorpus <- tm_map(dfCorpus, PlainTextDocument)
# Remove Pontuação
dfCorpus <- tm_map(dfCorpus, removePunctuation)
# Remover palavras especificas em ingles
dfCorpus <- tm_map(dfCorpus, removeWords, stopwords("english"))
# converter palavras similares em unicas
dfCorpus <- tm_map(dfCorpus, stemDocument)
#removendo palavras especificas
dfCorpus <- tm_map(dfCorpus, removeWords, c("the", "this", stopwords("english")))
#wordcount
wordcloud(dfCorpus, max.words = 100, random.order = FALSE)
head(df)
class(dfCorpus)
head(df)
#wordcount
wordcloud(dfCorpus, max.words = 100, random.order = FALSE)
df <- read.csv('questoes.csv', stringsAsFactors = FALSE)
head(df)
# Criando um Corpus
dfCorpus <- Corpus(VectorSource(df$Question))
class(dfCorpus)
# Convertendo Corpus em um documento de texto plano
dfCorpus <- tm_map(dfCorpus, PlainTextDocument)
# Remove pontuação
dfCorpus <- tm_map(dfCorpus, removePunctuation)
# Remover palavras específicas do inglês
dfCorpus <- tm_map(dfCorpus, removeWords, stopwords('english'))
# Neste processo, várias versões de uma palavras são convertidas em uma única instância
dfCorpus <- tm_map(dfCorpus, stemDocument)
# Removendo palavras específicas
dfCorpus <- tm_map(dfCorpus, removeWords, c('the', 'this', stopwords('english')))
# wordcloud
wordcloud(dfCorpus, max.words = 100, random.order = FALSE)
# wordcloud
wordcloud(dfCorpus, max.words = 100, random.order = FALSE)
dtm <- DocumentTermMatrix(dfCorpus)
arquivo <- "http://www.sthda.com/sthda/RDoc/example-files/martin-luther-king-i-have-a-dream-speech.txt"
texto <- readLines(arquivo)
# Carregando os dados como Corpus
docs <- Corpus(VectorSource(texto))
# Pré-processamento
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Converte o texto para minúsculo
docs <- tm_map(docs, content_transformer(tolower))
# Remove números
docs <- tm_map(docs, removeNumbers)
# Remove as palavras mais comuns do idioma inglês
docs <- tm_map(docs, removeWords, stopwords("english"))
# Você pode definir um vetor de palavras (stopwords) a serem removidas do texto
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove pontuação
docs <- tm_map(docs, removePunctuation)
# Elimina espaços extras
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
# wordcloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# Tabela de frequência
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = "freedom", corlimit = 0.3)
head(d, 10)
# Gráficos de barras com as palavras mais frequentes
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
col ="lightblue", main ="Most frequent words",
ylab = "Word frequencies")
reviews <- read.csv ("reviews.csv", stringsAsFactors=FALSE)
reviews <- read.csv ("reviews.csv", stringsAsFactors=FALSE)
str(reviews)
review_text <- paste(reviews$text, collapse=" ")
review_source <- VectorSource(review_text)
corpus <- tm_map(corpus, content_transformer(tolower))
#-----------------------------------------
reviews <- read.csv ("reviews.csv", stringsAsFactors=FALSE)
str(reviews)
review_text <- paste(reviews$text, collapse=" ")
review_source <- VectorSource(review_text)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(reviews, content_transformer(tolower))
corpus <- tm_map(review_source, content_transformer(tolower))
corpus <- tm_map(review_text, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(tolower))
#-----------------------------------------
reviews <- read.csv ("reviews.csv", stringsAsFactors=FALSE)
review_text <- paste(reviews$text, collapse=" ")
review_source <- VectorSource(review_text)
corpus <- tm_map(reviews, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(review_text, content_transformer(tolower))
corpus <- tm_map(review_source, content_transformer(tolower))
